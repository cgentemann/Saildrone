{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment from shane\n",
    "You should consider using multitaper estimates with Slepian (dpss) tapers. They have much better spectral characteristics and compared to the Welchâ€™s method it allows you to estimate spectral density at the frequency/wavenumber corresponding to the total length of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seawater as sw\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import scipy.ndimage\n",
    "import matplotlib.colors as colors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#data directory for saildrone data\n",
    "#more on the data here: https://podaac.jpl.nasa.gov/dataset/SAILDRONE_ATOMIC\n",
    "# DOI = 10.5067/SDRON-ATOM0\n",
    "data_dir = './../../paper_software/2020_ATOMIC_bjorn/data/'\n",
    "saildrone_filenames = [x for x in glob(data_dir+'saildrone*.nc')]\n",
    "\n",
    "#output\n",
    "figs_dir = './figures/'\n",
    "\n",
    "#subroutines for calculating PSD & making plot\n",
    "def spectrum(data_in):\n",
    "    #calculate PSD for each USV\n",
    "    data_all=[]\n",
    "    for iusv in range(3):\n",
    "        ds_usv = data_in.isel(trajectory=iusv)\n",
    "        ds2 = ds_usv.assign_coords(dist_total = ds_usv.dist_total)\n",
    "        ds3 = ds2.swap_dims({'time':'dist_total'})\n",
    "        dist_interp = np.arange(ds2.dist_total[0],ds2.dist_total[-1],0.08)\n",
    "        ds4 = ds3.interp(dist_total=dist_interp)\n",
    "        den = ds4.density_mean.interpolate_na(dim='dist_total')\n",
    "        den = den.where(np.isfinite(den),drop=True)\n",
    "        ds4_detrend = signal.detrend(den)\n",
    "        ds4_detrend_smooth = ds4_detrend\n",
    "#        ds4_detrend_smooth = scipy.ndimage.filters.gaussian_filter1d(ds4_detrend, sigma=25)\n",
    "        freq, Pxx_den = signal.periodogram(ds4_detrend_smooth,1/.080)  #fs = sampled at .08km or 80m\n",
    "        freq2, Pxx_den2 = signal.welch(ds4_detrend_smooth,1/.080,nperseg=1024*30)  #fs = sampled at .08km or 80m\n",
    "        if iusv==0:\n",
    "            ps_all=Pxx_den[0:10000]\n",
    "            ps_all_welch=Pxx_den2[0:10000]\n",
    "        else:\n",
    "            ps_all = np.vstack([ps_all,Pxx_den[0:10000]])\n",
    "            ps_all_welch = np.vstack([ps_all_welch,Pxx_den2[0:10000]])    \n",
    "    Pxx_den = np.mean(ps_all,axis=0)\n",
    "    Pxx_den_welch = np.mean(ps_all_welch,axis=0)\n",
    "    return freq,freq2,Pxx_den,Pxx_den_welch\n",
    "\n",
    "def cal_pdf(data_in): \n",
    "    #make arrays for sampling at different length scales\n",
    "    length_scale = np.arange(.1,200,1)\n",
    "    # create the empty data arrays to store the normalized histograms (normalized the *100 for percentage count)\n",
    "    xx_in = np.arange(0,.2,.001)\n",
    "    xx_in2 = np.arange(0,.2-.001,.001)\n",
    "    data = np.zeros((len(length_scale),len(xx_in2)))\n",
    "    ddn=xr.DataArray(data,dims=('length_scale','gradx'),coords={'length_scale':length_scale,'gradx':xx_in2})\n",
    "    for iusv in range(3):\n",
    "        ds_usv = data_in.isel(trajectory=iusv)\n",
    "        ds2 = ds_usv.assign_coords(dist_total = ds_usv.dist_total)  #add dist traveled coordinate\n",
    "        ds3 = ds2.swap_dims({'time':'dist_total'})                  #swap from time to distance traveled\n",
    "        for ilen2,len2 in enumerate(length_scale):\n",
    "            dist_interp = np.arange(ds2.dist_total[0],ds2.dist_total[-1],len2)\n",
    "            ds4 = ds3.interp(dist_total=dist_interp)       \n",
    "            den_grad =  np.abs(np.gradient(ds4.density_mean)/len2)\n",
    "            result,xx = np.histogram(den_grad,bins=xx_in)\n",
    "            ddn[ilen2,:]=ddn[ilen2,:]+result\n",
    "    for ilen2,len2 in enumerate(length_scale):\n",
    "        ddn[ilen2,:]=ddn[ilen2,:]/sum(ddn[ilen2,:])*100  #normalize & turn into percent\n",
    "\n",
    "    return ddn\n",
    "\n",
    "\n",
    "def psd_fig(f,data_in,Pxx_den,text1,fout,ifit):\n",
    "   \n",
    "    length_scale = np.arange(.1,200,1)\n",
    "    xx_in = np.arange(0,.2,.001)\n",
    "    xx_in2 = np.arange(0,.2-.001,.001)\n",
    "    print(len(length_scale),len(xx_in))\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    tem=data_in\n",
    "    tem = tem.where(tem>.003)\n",
    "    Z=tem.T\n",
    "    ax = plt.pcolormesh(length_scale,xx_in2,Z, norm=colors.LogNorm(vmin=Z.min(), vmax=Z.max()),vmin=.01,vmax=100,cmap='hot')\n",
    "    plt.text(10,0.179,'(a)'+text1,fontsize=16,color='k')\n",
    "    plt.xlabel('Length scale (km)',fontsize=16)\n",
    "    plt.ylabel('Density gradient (kg m$^{-3}$ km$^{-1}$)',fontsize=16)\n",
    "    cb = plt.colorbar()\n",
    "    cb.set_label(label='Percent count',fontsize=16)\n",
    "\n",
    "    plt.axes([.33, .47, .4, .4])\n",
    "\n",
    "    #cal PSD\n",
    "    #Pxx_den = np.mean(psd_in,axis=0)\n",
    "    #f, Pxx_den = signal.periodogram(Pxx_den,1/.080)  #fs = sampled at .08km or 80m\n",
    "    istart,iend=10,5550\n",
    "    plt.loglog(f[istart:iend], Pxx_den[istart:iend])\n",
    "\n",
    "    #linear regression to PSD\n",
    "    istart,iend=8,ifit\n",
    "    XX = np.log(f[istart:iend])\n",
    "    YY = np.log(Pxx_den[istart:iend])\n",
    "    reg = LinearRegression().fit(XX.reshape(-1, 1), YY)\n",
    "    a = float(reg.coef_)\n",
    "    b = -1*float(reg.intercept_)\n",
    "    plt.loglog(f[istart:iend], f[istart:iend]**(a)/np.exp(b),'r') #test from fit\n",
    "    slp_str = 'slope = '+\"{:.1f}\".format(a)\n",
    "    plt.text(.02,10,slp_str,fontsize=16,color='r')\n",
    "    plt.ylim([10e-6,10e1])\n",
    "    plt.xlim([10e-4,10e-1])\n",
    "    plt.xticks(ticks=[.001,.01,.1,1],labels=['1000','100','10','1'])\n",
    "\n",
    "    plt.text(.0011,10,'(b)',fontsize=16,color='k')\n",
    "    #plt.xlabel('Wavenumber (cpkm)')\n",
    "    plt.xlabel('Wavelength (km)')\n",
    "    plt.ylabel('PSD ((kg m$^{-3}$)$^2$ cpkm$^{-1}$]')\n",
    "    plt.grid()\n",
    "    #plt.savefig(figs_dir+fout)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in USV data for all 3 Saildrone\n",
    "- caluclate density and wind speed\n",
    "- caluclate distance between successive obs\n",
    "- caluculate total cumulative distance\n",
    "- switch from time to cumulative distance as index\n",
    "- interpolate data onto grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gentemann\\Miniconda3\\envs\\satenv\\lib\\site-packages\\xarray\\core\\nanops.py:142: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis=axis, dtype=dtype)\n",
      "C:\\Users\\gentemann\\Miniconda3\\envs\\satenv\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\gentemann\\Miniconda3\\envs\\satenv\\lib\\site-packages\\xarray\\core\\nanops.py:142: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis=axis, dtype=dtype)\n",
      "C:\\Users\\gentemann\\Miniconda3\\envs\\satenv\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\gentemann\\Miniconda3\\envs\\satenv\\lib\\site-packages\\xarray\\core\\nanops.py:142: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis=axis, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "ds=[]\n",
    "for iusv in range(3):\n",
    "    fname=saildrone_filenames[iusv]\n",
    "    ds_usv=xr.open_dataset(fname).isel(trajectory=0).swap_dims({'obs':'time'})\n",
    "    ds_usv.close()\n",
    "\n",
    "#    #make diruanl plot\n",
    "    xlon=ds_usv.longitude.data\n",
    "    time_offset_to_lmt=(xlon/360.)*24.*60\n",
    "    tem = ds_usv.time+time_offset_to_lmt*np.timedelta64(1,'m')# dt.timedelta(seconds=1)\n",
    "    ds_usv['tlmt']=tem\n",
    "\n",
    "    ds_usv2= ds_usv.swap_dims({'time':'tlmt'})\n",
    "    ds_usv2a = ds_usv2.where(ds_usv2.tlmt.dt.hour==6)\n",
    "    dymn = ds_usv2a.groupby(\"tlmt.dayofyear\").mean()\n",
    "    ds_usv3 = ds_usv2.groupby(\"tlmt.dayofyear\") - dymn\n",
    "   \n",
    "    ds_usv['TEMP_AIR_MEAN_DW'] = ds_usv3.swap_dims({'tlmt':'time'}).drop({'tlmt'}).TEMP_AIR_MEAN\n",
    "    ds_usv['TEMP_SBE37_MEAN_DW'] = ds_usv3.swap_dims({'tlmt':'time'}).drop({'tlmt'}).TEMP_SBE37_MEAN\n",
    "    ds_usv['wspd']=np.sqrt(ds_usv.UWND_MEAN**2+ds_usv.VWND_MEAN**2)   \n",
    "    tem=sw.dens0(ds_usv.SAL_SBE37_MEAN,ds_usv.TEMP_SBE37_MEAN)\n",
    "    ds_usv['density_mean']=xr.DataArray(tem,dims=('time'),coords={'time':ds_usv.time})\n",
    "    tem=sw.alpha(ds_usv.SAL_SBE37_MEAN,ds_usv.TEMP_SBE37_MEAN,ds_usv.BARO_PRES_MEAN*0) #pressure =0 at surface\n",
    "    ds_usv['alpha_ME']=xr.DataArray(tem,dims=('time'),coords={'time':ds_usv.time})\n",
    "    tem=sw.beta(ds_usv.SAL_SBE37_MEAN,ds_usv.TEMP_SBE37_MEAN,ds_usv.BARO_PRES_MEAN*0) #pressure =0 at surface\n",
    "    ds_usv['beta_MEAN']=xr.DataArray(tem,dims=('time'),coords={'time':ds_usv.time})\n",
    "    ds_usv['latitude']=ds_usv.latitude.interpolate_na(dim='time')\n",
    "    ds_usv['longitude']=ds_usv.longitude.interpolate_na(dim='time')\n",
    "    xlat=ds_usv.latitude\n",
    "    xlon=ds_usv.longitude\n",
    "    dkm2 = abs(np.abs((((xlon[1:].data-xlon[0:-1].data)**2+(xlat[1:].data-xlat[0:-1].data)**2)**.5)*110.567*np.cos(np.pi*xlat[1:].data/180)))\n",
    "    dkm2=np.append(dkm2,dkm2[66238]) #add on last point\n",
    "    dkm3 = dkm2.cumsum()\n",
    "    ds_usv['dist_total']=xr.DataArray(dkm3,dims=('time'),coords={'time':ds_usv.time})\n",
    "    ds_usv['dist_between']=xr.DataArray(dkm2,dims=('time'),coords={'time':ds_usv.time})\n",
    "    if iusv==0:\n",
    "        ds = ds_usv\n",
    "    else:\n",
    "        ds = xr.concat([ds,ds_usv],dim='trajectory')\n",
    "ds_saildrone = ds.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq_usv,freq2_usv,Pxx_den_usv,Pxx_den_welch_usv = spectrum(ds)\n",
    "\n",
    "ddn_usv = cal_pdf(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "psd_fig(freq_usv,ddn_usv,Pxx_den_usv,'Saildrone','PSD_den_grad_usv.png',300)\n",
    "psd_fig(freq_usv,ddn_usv,Pxx_den_usv,'Saildrone','PSD_den_grad_usv.pdf',300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
